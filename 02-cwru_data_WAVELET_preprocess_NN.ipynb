{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01-cwru data WAVELET preprocess NN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python385jvsc74a57bd0b9613c3141d1bd96539dd314c0a50d7a38025bd72d1f893a8e76c8ec6c05357d",
      "display_name": "Python 3.8.5 64-bit ('virenv4': conda)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5lvdRe6OTck"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pywt\n",
        "import glob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DtnuL3CAOjUA",
        "outputId": "6bb802f1-6ee0-4da4-b365-143bd59cc9dd"
      },
      "source": [
        "files = glob.glob('cwru-selected-data/*/*.csv')\n",
        "files"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['cwru-selected-data/cwru-selected-data/B021_1_227.csv',\n",
              " 'cwru-selected-data/cwru-selected-data/IR021_1_214.csv',\n",
              " 'cwru-selected-data/cwru-selected-data/IR014_1_175.csv',\n",
              " 'cwru-selected-data/cwru-selected-data/IR007_1_110.csv',\n",
              " 'cwru-selected-data/cwru-selected-data/Time_Normal_1_098.csv',\n",
              " 'cwru-selected-data/cwru-selected-data/OR021_6_1_239.csv',\n",
              " 'cwru-selected-data/cwru-selected-data/B014_1_190.csv',\n",
              " 'cwru-selected-data/cwru-selected-data/B007_1_123.csv',\n",
              " 'cwru-selected-data/cwru-selected-data/OR007_6_1_136.csv',\n",
              " 'cwru-selected-data/cwru-selected-data/OR014_6_1_202.csv']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-9btUJAOsWh",
        "outputId": "c951e80b-3e7a-4bdb-b502-8d528489d570"
      },
      "source": [
        "\n",
        "files = glob.glob('cwru-selected-data/*/*.csv')\n",
        "data = np.empty((460*10,32,32))\n",
        "data[:] = np.nan\n",
        "\n",
        "count = 0\n",
        "for file in files:\n",
        "    temp = pd.read_csv(file, header = None)\n",
        "    for j in range(460):\n",
        "        segment = temp[0][(j*1024):((j+1)*1024)].values\n",
        "        coefs,_ = pywt.cwt(segment,np.arange(1,2049,32), 'gaus1')\n",
        "        data[count,:,:] = tf.reshape(tf.image.resize(coefs.reshape((64,1024,1)),(32,32)),(32,32))\n",
        "        if (count % 100) == 0:\n",
        "            print(f\"{count} data points extracted\")\n",
        "        count = count + 1\n",
        "fault_types = ['Ball_007', 'Ball_014', 'Ball_021', 'IR_007', 'IR_014', 'IR_021', 'OR_007','OR_014', 'OR_021', 'Normal']\n",
        "labels = np.repeat(fault_types, 460)\n",
        "# Save wavelet data\n",
        "np.savez('CWRU_48k_load_1_CNN_wavelet_data', data = data, labels = labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 data points extracted\n",
            "100 data points extracted\n",
            "200 data points extracted\n",
            "300 data points extracted\n",
            "400 data points extracted\n",
            "500 data points extracted\n",
            "600 data points extracted\n",
            "700 data points extracted\n",
            "800 data points extracted\n",
            "900 data points extracted\n",
            "1000 data points extracted\n",
            "1100 data points extracted\n",
            "1200 data points extracted\n",
            "1300 data points extracted\n",
            "1400 data points extracted\n",
            "1500 data points extracted\n",
            "1600 data points extracted\n",
            "1700 data points extracted\n",
            "1800 data points extracted\n",
            "1900 data points extracted\n",
            "2000 data points extracted\n",
            "2100 data points extracted\n",
            "2100 data points extracted\n",
            "2200 data points extracted\n",
            "2200 data points extracted\n",
            "2300 data points extracted\n",
            "2300 data points extracted\n",
            "2400 data points extracted\n",
            "2400 data points extracted\n",
            "2500 data points extracted\n",
            "2500 data points extracted\n",
            "2600 data points extracted\n",
            "2600 data points extracted\n",
            "2700 data points extracted\n",
            "2700 data points extracted\n",
            "2800 data points extracted\n",
            "2800 data points extracted\n",
            "2900 data points extracted\n",
            "2900 data points extracted\n",
            "3000 data points extracted\n",
            "3000 data points extracted\n",
            "3100 data points extracted\n",
            "3100 data points extracted\n",
            "3200 data points extracted\n",
            "3200 data points extracted\n",
            "3300 data points extracted\n",
            "3300 data points extracted\n",
            "3400 data points extracted\n",
            "3400 data points extracted\n",
            "3500 data points extracted\n",
            "3500 data points extracted\n",
            "3600 data points extracted\n",
            "3600 data points extracted\n",
            "3700 data points extracted\n",
            "3700 data points extracted\n",
            "3800 data points extracted\n",
            "3800 data points extracted\n",
            "3900 data points extracted\n",
            "3900 data points extracted\n",
            "4000 data points extracted\n",
            "4000 data points extracted\n",
            "4100 data points extracted\n",
            "4100 data points extracted\n",
            "4200 data points extracted\n",
            "4200 data points extracted\n",
            "4300 data points extracted\n",
            "4300 data points extracted\n",
            "4400 data points extracted\n",
            "4400 data points extracted\n",
            "4500 data points extracted\n",
            "4500 data points extracted\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-q_61r58O4_o"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}